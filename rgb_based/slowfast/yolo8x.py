import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import cv2
import os
import glob
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import json
from torchvision.transforms import Compose, Lambda, functional, transforms
from torchvision.transforms._transforms_video import (
    CenterCropVideo,
    NormalizeVideo,
)
from pytorchvideo.data.encoded_video import EncodedVideo
from pytorchvideo.transforms import (
    ApplyTransformToKey,
    ShortSideScale,
    UniformTemporalSubsample,
)
from pytorchvideo.models.head import ResNetBasicHead
from tqdm import tqdm
import time
import datetime
import argparse
import copy
import torch.nn.functional as F
from collections import defaultdict
from ultralytics import YOLO

# ç’°å¢ƒè¨­å®š
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"  # Windowsç’°å¢ƒã§ã®ã‚¨ãƒ©ãƒ¼å›é¿

# AVAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹ï¼ˆ80ã‚¯ãƒ©ã‚¹ï¼‰
AVA_ACTIONS = [
    "bend/bow", "crawl", "crouch/kneel", "dance", "fall down", "get up", "jump/leap", "lie/sleep", "martial art", "run/jog",
    "sit", "stand", "swim", "walk", "answer phone", "brush teeth", "carry/hold", "catch", "chop", "climb",
    "clink glass", "close", "cook", "cut", "dig", "dress/put on", "drink", "drive", "eat", "enter",
    "exit", "extract", "fishing", "hit", "kick", "lift/pick up", "listen", "open", "paint", "play board game",
    "play instrument", "play sports", "point", "pour", "press", "pull", "push", "put down", "read", "ride",
    "row boat", "sail boat", "shoot", "shovel", "smoke", "stir", "take photo", "text on phone", "throw", "touch",
    "turn", "watch", "work on computer", "write", "fight/hit", "give/serve", "grab", "hand clap", "hand shake", "hand wave",
    "hug", "kiss", "lift person", "listen to person", "play with kids", "push person", "sing to", "take from person", "talk to", "watch person"
]

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹è¡Œå‹•ã‚¯ãƒ©ã‚¹ï¼ˆé¸æŠã—ãŸã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
TARGET_ACTIONS = [
    "stand", "sit", "walk", "run/jog", "carry/hold", "fight/hit", "fall down", "lie/sleep", 
    "dance", "talk to", "eat", "work on computer", "read", "write", "hand wave"
]

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°ã®ä½œæˆ
TARGET_ACTION_INDICES = [AVA_ACTIONS.index(action) for action in TARGET_ACTIONS]

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
BATCH_SIZE = 8
NUM_EPOCHS = 5
LEARNING_RATE = 0.0001
WEIGHT_DECAY = 1e-4
TRAIN_VAL_SPLIT = 0.8  # è¨“ç·´:æ¤œè¨¼ã®æ¯”ç‡

# SlowFastç”¨ã®è¨­å®š
SIDE_SIZE = 256
MEAN = [0.45, 0.45, 0.45]
STD = [0.225, 0.225, 0.225]
CROP_SIZE = 256
NUM_FRAMES = 32 # å¤‰æ›´ä¸å¯
SAMPLING_RATE = 4
FRAMES_PER_SECOND = 30
SLOWFAST_ALPHA = 4 # å¤‰æ›´ä¸å¯
CLIP_DURATION = (NUM_FRAMES * SAMPLING_RATE) / FRAMES_PER_SECOND

# æ¤œå‡ºé–¢é€£ã®è¨­å®š
PERSON_THRESHOLD = 0.3  # äººç‰©æ¤œå‡ºã®é–¾å€¤
ACTION_THRESHOLD = 0.4  # è¡Œå‹•æ¤œå‡ºã®é–¾å€¤
NUM_PERSON_CLASSES = 1  # AVAãƒ¢ãƒ‡ãƒ«ã¯äººç‰©ã‚¯ãƒ©ã‚¹ã®ã¿æ¤œå‡º
NUM_ACTION_CLASSES = 80  # AVAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¡Œå‹•ã‚¯ãƒ©ã‚¹æ•°
DETECTION_INTERVAL = 30  # æ¤œå‡ºé–“éš”ï¼ˆç´„1ç§’ï¼‰

class PackPathway(torch.nn.Module):
    """
    å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’SlowFastãƒ¢ãƒ‡ãƒ«ç”¨ã«å¤‰æ›ã™ã‚‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ 
    """
    def __init__(self, alpha):
        super().__init__()
        self.alpha = alpha
            
    def forward(self, frames: torch.Tensor):
        fast_pathway = frames
        # Slow Pathway
        if not isinstance(frames, torch.Tensor):
            frames = torch.from_numpy(frames).float() if isinstance(frames, np.ndarray) else frames
            
        slow_pathway = torch.index_select(
            frames,
            1, # Time dimension
            torch.linspace(
                0, frames.shape[1] - 1, frames.shape[1] // self.alpha
            ).long(),
        )
        frame_list = [slow_pathway, fast_pathway]
        return frame_list

def create_detection_transform(alpha=SLOWFAST_ALPHA):
    """SlowFastæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ç”¨ã®å¤‰æ›ã‚’ä½œæˆ"""
    transform = ApplyTransformToKey(
        key="video",
        transform=Compose(
            [
                UniformTemporalSubsample(NUM_FRAMES),
                Lambda(lambda x: x / 255.0),
                NormalizeVideo(MEAN, STD),
                ShortSideScale(size=SIDE_SIZE),
                CenterCropVideo(CROP_SIZE),
                PackPathway(alpha=alpha)
            ]
        ),
    )
    return transform

class SlowFastActionDetectionModel(nn.Module):
    """
    SlowFastè¡Œå‹•æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
    """
    def __init__(self, target_actions=None):
        super().__init__()
        
        self.target_actions = target_actions
        self.action_indices = None
        if target_actions:
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
            self.action_indices = [AVA_ACTIONS.index(action) for action in target_actions]
        
        # SlowFastæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        print("SlowFastæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        try:
            self.model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50_detection', pretrained=True)
            print("SlowFast R50æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            self.model_name = 'slowfast_r50_detection'
        except Exception as e:
            print(f"R50æ¤œå‡ºãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            try:
                print("SlowFast R101æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã¿ã¾ã™...")
                self.model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101_detection', pretrained=True)
                print("SlowFast R101æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                self.model_name = 'slowfast_r101_detection'
            except Exception as e2:
                print(f"R101æ¤œå‡ºãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e2}")
                raise RuntimeError("åˆ©ç”¨å¯èƒ½ãªSlowFastæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’è¡¨ç¤º
        self._analyze_model_structure()
    
    def _analyze_model_structure(self):
        """ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã‚’åˆ†æã—ã¦è¡¨ç¤º"""
        print("\nãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®åˆ†æ:")
        
        # ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç¢ºèª
        if hasattr(self.model, 'detection_head'):
            print("æ¤œå‡ºãƒ˜ãƒƒãƒ‰:", type(self.model.detection_head))
            
            if hasattr(self.model.detection_head, 'bbox_head'):
                print("ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ãƒ˜ãƒƒãƒ‰:", type(self.model.detection_head.bbox_head))
                if isinstance(self.model.detection_head.bbox_head, ResNetBasicHead):
                    print(f"ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ãƒ˜ãƒƒãƒ‰å‡ºåŠ›ã‚µã‚¤ã‚º: {self.model.detection_head.bbox_head.proj.out_features}")
            
            if hasattr(self.model.detection_head, 'cls_head'):
                print("åˆ†é¡ãƒ˜ãƒƒãƒ‰:", type(self.model.detection_head.cls_head))
                if isinstance(self.model.detection_head.cls_head, ResNetBasicHead):
                    print(f"åˆ†é¡ãƒ˜ãƒƒãƒ‰å‡ºåŠ›ã‚µã‚¤ã‚º: {self.model.detection_head.cls_head.proj.out_features}")
        else:
            print("æ¤œå‡ºãƒ˜ãƒƒãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è©³ç´°ãªæ§‹é€ ã‚’è¡¨ç¤ºã—ã¾ã™:")
            self._print_model_structure()
    
    def _print_model_structure(self):
        """ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’è©³ç´°ã«å‡ºåŠ›ã™ã‚‹è£œåŠ©é–¢æ•°"""
        print("è©³ç´°ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ :")
        for name, module in self.model.named_children():
            print(f"Module '{name}':", module)
    
    def forward(self, x, bboxes=None):
        """
        ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­
        å…¥åŠ›: 
        - x: [slow_pathway, fast_pathway]
        - bboxes: ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ [N, 4] (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        å‡ºåŠ›: æ¤œå‡ºçµæœï¼ˆè¡Œå‹•ã‚¯ãƒ©ã‚¹ã®ãƒ­ã‚¸ãƒƒãƒˆï¼‰
        """
        # bboxesãŒNoneã®å ´åˆã¯ã€ãƒ€ãƒŸãƒ¼ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ
        if bboxes is None:
            device = x[0].device if isinstance(x, list) else x.device
            bboxes = torch.tensor([[64, 64, 192, 192]], dtype=torch.float32).to(device)
        
        # å…¥åŠ›ã®å½¢çŠ¶ã‚’ç¢ºèªãƒ»ä¿®æ­£
        if isinstance(x, list) and len(x) == 2:
            slow_pathway, fast_pathway = x
            
            # 5Då½¢çŠ¶ãƒã‚§ãƒƒã‚¯ (batch_size, channels, temporal, height, width)
            if slow_pathway.dim() == 4:
                slow_pathway = slow_pathway.unsqueeze(0)  # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ 
                print("Slow pathwayã«ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            if fast_pathway.dim() == 4:
                fast_pathway = fast_pathway.unsqueeze(0)  # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ 
                print("Fast pathwayã«ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            
            x = [slow_pathway, fast_pathway]
            
            # å½¢çŠ¶ã‚’ç¢ºèª
            print(f"Slow pathway å½¢çŠ¶: {slow_pathway.shape}")
            print(f"Fast pathway å½¢çŠ¶: {fast_pathway.shape}")
            print(f"Bounding boxes å½¢çŠ¶: {bboxes.shape}")
        
        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®å½¢çŠ¶ã‚’ä¿®æ­£ [N, 4] -> [N, 5]
        # AVAãƒ¢ãƒ‡ãƒ«ã¯ [batch_index, x1, y1, x2, y2] å½¢å¼ã‚’æœŸå¾…
        if bboxes.shape[1] == 4:
            batch_indices = torch.zeros((bboxes.shape[0], 1), device=bboxes.device)
            bboxes = torch.cat([batch_indices, bboxes], dim=1)
            print(f"ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’AVAå½¢å¼ã«å¤‰æ›: {bboxes.shape}")
        
        try:
            # AVAæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ç”¨ã®æ­£ã—ã„å‘¼ã³å‡ºã—æ–¹æ³•
            if hasattr(self.model, 'extract_features'):
                # ç‰¹å¾´æŠ½å‡º
                print("ç‰¹å¾´æŠ½å‡ºã‚’å®Ÿè¡Œä¸­...")
                features = self.model.extract_features(x)
                print(f"æŠ½å‡ºã•ã‚ŒãŸç‰¹å¾´ã®å½¢çŠ¶: {[f.shape for f in features] if isinstance(features, list) else features.shape}")
                
                # æ¤œå‡ºãƒ˜ãƒƒãƒ‰ã«ç‰¹å¾´ã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æ¸¡ã™
                print("æ¤œå‡ºãƒ˜ãƒƒãƒ‰ã‚’å®Ÿè¡Œä¸­...")
                preds = self.model.detection_head(features, bboxes)
                return preds
            else:
                # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å«ã‚ã¦ç›´æ¥å‘¼ã³å‡ºã—
                print("ç›´æ¥ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—ã‚’å®Ÿè¡Œä¸­...")
                return self.model(x, bboxes)
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã§å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"å…¥åŠ›å½¢çŠ¶ã®è©³ç´° - Slow: {x[0].shape}, Fast: {x[1].shape}, BBoxes: {bboxes.shape}")
            
            # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±
            import traceback
            traceback.print_exc()
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ€ãƒŸãƒ¼ã®å‡ºåŠ›ã‚’è¿”ã™
            batch_size = bboxes.shape[0] if bboxes is not None else 1
            device = x[0].device if isinstance(x, list) else x.device
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã‚’ç”Ÿæˆä¸­ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}ï¼‰")
            return torch.randn(batch_size, 80, device=device)  # 80ã¯è¡Œå‹•ã‚¯ãƒ©ã‚¹æ•°

def preprocess_video_for_detection(video_clip):
    """
    å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’SlowFastæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ç”¨ã«å‰å‡¦ç†
    """
    # NumPyé…åˆ—ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ï¼ˆ[T, H, W, C] -> [C, T, H, W]ï¼‰
    video_tensor = torch.from_numpy(video_clip.transpose(3, 0, 1, 2)).float()
    
    # æ­£è¦åŒ–
    video_tensor = video_tensor / 255.0
    
    # å¹³å‡ã¨æ¨™æº–åå·®ã§æ­£è¦åŒ–
    for i in range(3):
        video_tensor[i] = (video_tensor[i] - MEAN[i]) / STD[i]
    
    # SlowFastã®2ã¤ã®ãƒ‘ã‚¹ã‚¦ã‚§ã‚¤ã«å¤‰æ›
    # Fast pathway ã¯å…¨ãƒ•ãƒ¬ãƒ¼ãƒ 
    fast_pathway = video_tensor
    
    # Slow pathway ã¯é–“å¼•ã„ãŸãƒ•ãƒ¬ãƒ¼ãƒ 
    slow_indices = torch.linspace(
        0, video_tensor.shape[1] - 1, video_tensor.shape[1] // SLOWFAST_ALPHA
    ).long()
    slow_pathway = torch.index_select(video_tensor, 1, slow_indices)
    
    # é‡è¦: ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ ã—ã¦5Dãƒ†ãƒ³ã‚½ãƒ«ã«ã™ã‚‹
    # [C, T, H, W] -> [1, C, T, H, W]
    slow_pathway = slow_pathway.unsqueeze(0)
    fast_pathway = fast_pathway.unsqueeze(0)
    
    return [slow_pathway, fast_pathway]

def process_detections(pred_boxes, pred_scores, action_scores, image_size, 
                      score_threshold=PERSON_THRESHOLD, action_threshold=ACTION_THRESHOLD):
    """
    æ¤œå‡ºçµæœã‚’å‡¦ç†ã—ã¦ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨è¡Œå‹•ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    """
    results = []
    if pred_boxes is None or len(pred_boxes) == 0:
        print("âš ï¸ pred_boxes ãŒç©ºã§ã™")
        return results
    
    print(f"ğŸ” æ¤œå‡ºå‡¦ç†é–‹å§‹:")
    print(f"  - pred_boxes: {pred_boxes.shape}")
    print(f"  - pred_scores: {pred_scores.shape}")  
    print(f"  - action_scores: {action_scores.shape}")
    
    # å…¨ã¦ã®æ¤œå‡ºã‚’å‡¦ç†
    for i in range(len(pred_boxes)):
        print(f"ğŸ“¦ æ¤œå‡º {i+1}/{len(pred_boxes)} ã‚’å‡¦ç†ä¸­...")
        
        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®å–å¾—
        box = pred_boxes[i].cpu().numpy() if torch.is_tensor(pred_boxes[i]) else pred_boxes[i]
        print(f"   SlowFastãƒœãƒƒã‚¯ã‚¹åº§æ¨™: {box}")
        
        # åº§æ¨™ã‚’è¡¨ç¤ºã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã«ç›´æ¥å¤‰æ›
        # CROP_SIZE (256x256) -> è¡¨ç¤ºã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ (960x720)
        display_x1 = int(box[0] * 960 / CROP_SIZE)
        display_y1 = int(box[1] * 720 / CROP_SIZE)  
        display_x2 = int(box[2] * 960 / CROP_SIZE)
        display_y2 = int(box[3] * 720 / CROP_SIZE)
        
        # åº§æ¨™ã®å¢ƒç•Œãƒã‚§ãƒƒã‚¯
        display_x1 = max(0, min(display_x1, 960))
        display_y1 = max(0, min(display_y1, 720))
        display_x2 = max(0, min(display_x2, 960))
        display_y2 = max(0, min(display_y2, 720))
        
        display_box = [display_x1, display_y1, display_x2, display_y2]
        print(f"   è¡¨ç¤ºãƒœãƒƒã‚¯ã‚¹åº§æ¨™: {display_box}")
        
        # äººç‰©ã‚¹ã‚³ã‚¢ã®å–å¾—
        person_score = pred_scores[i].item() if i < len(pred_scores) else 0.9
        print(f"   äººç‰©ã‚¹ã‚³ã‚¢: {person_score}")
        
        # è¡Œå‹•ã‚¹ã‚³ã‚¢ã®å‡¦ç†
        if i < action_scores.shape[0]:
            action_logits = action_scores[i]
        else:
            action_logits = action_scores[0]  # æœ€åˆã®ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
        
        # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‚’é©ç”¨ã—ã¦ç¢ºç‡ã«å¤‰æ›
        action_probs = torch.sigmoid(action_logits)
        
        # é–¾å€¤ã‚’è¶…ãˆã‚‹è¡Œå‹•ã‚’é¸æŠã€ãªã‘ã‚Œã°ä¸Šä½3ã¤ã‚’é¸æŠ
        high_confidence_actions = []
        for j, prob in enumerate(action_probs):
            if prob.item() > action_threshold and j < len(AVA_ACTIONS):
                high_confidence_actions.append((j, prob.item()))
        
        # é–¾å€¤ã‚’è¶…ãˆã‚‹è¡Œå‹•ãŒãªã„å ´åˆã¯ä¸Šä½3ã¤ã‚’é¸æŠ
        if not high_confidence_actions:
            top_k = 3
            top_values, top_indices = torch.topk(action_probs, top_k)
            actions = []
            for j in range(top_k):
                action_id = top_indices[j].item()
                action_score = top_values[j].item()
                if action_id < len(AVA_ACTIONS):
                    action_name = AVA_ACTIONS[action_id]
                    actions.append((action_name, action_score))
                    print(f"   è¡Œå‹• {j+1}: {action_name} (ã‚¹ã‚³ã‚¢: {action_score:.3f})")
        else:
            # é–¾å€¤ã‚’è¶…ãˆã‚‹è¡Œå‹•ã‚’ä½¿ç”¨
            actions = []
            high_confidence_actions.sort(key=lambda x: x[1], reverse=True)  # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            for action_id, action_score in high_confidence_actions[:5]:  # ä¸Šä½5ã¤ã¾ã§
                action_name = AVA_ACTIONS[action_id]
                actions.append((action_name, action_score))
                print(f"   é«˜ä¿¡é ¼åº¦è¡Œå‹•: {action_name} (ã‚¹ã‚³ã‚¢: {action_score:.3f})")
        
        # çµæœã®è¿½åŠ 
        result = {
            "box": display_box,  # è¡¨ç¤ºç”¨ã«å¤‰æ›æ¸ˆã¿ã®åº§æ¨™
            "person_score": person_score,
            "actions": actions
        }
        results.append(result)
        print(f"   âœ… æ¤œå‡ºçµæœ {i+1} ã‚’è¿½åŠ ")
    
    print(f"ğŸ¯ å‡¦ç†å®Œäº†: {len(results)}å€‹ã®æ¤œå‡ºçµæœ")
    return results

def visualize_detections(frame, detections, target_actions=None):
    """
    æ¤œå‡ºçµæœã‚’å¯è¦–åŒ–ã—ã¦æç”»æ¸ˆã¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™
    """
    vis_frame = frame.copy()
    
    if not detections:
        return vis_frame
    
    print(f"ğŸ¨ å¯è¦–åŒ–é–‹å§‹: {len(detections)}å€‹ã®æ¤œå‡ºçµæœã‚’å‡¦ç†")
    
    # å„æ¤œå‡ºã«å¯¾ã—ã¦å‡¦ç†
    for i, det in enumerate(detections):
        try:
            box = det["box"]
            x1, y1, x2, y2 = [max(0, min(int(coord), frame.shape[1] if coord in [box[0], box[2]] else frame.shape[0])) for coord in box]
            
            print(f"  æ¤œå‡º{i+1}: ãƒœãƒƒã‚¯ã‚¹({x1}, {y1}, {x2}, {y2})")
            
            # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»ï¼ˆå¤ªã„ç·‘ç·šï¼‰
            cv2.rectangle(vis_frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
            
            # äººç‰©ã‚¹ã‚³ã‚¢ã‚’æç”»
            person_text = f"Person: {det['person_score']:.2f}"
            cv2.putText(vis_frame, person_text, (x1, max(y1 - 15, 20)),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 3)
            
            # è¡Œå‹•ãƒ©ãƒ™ãƒ«ã‚’æç”»ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰
            y_offset = 30
            for j, (action, score) in enumerate(det["actions"][:3]):
                action_text = f"{action}: {score:.2f}"
                text_color = (0, 0, 255) if score > 0.5 else (0, 255, 255)
                
                text_y = min(y1 + y_offset, frame.shape[0] - 15)
                cv2.putText(vis_frame, action_text, (x1, text_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)
                y_offset += 30
                print(f"    è¡Œå‹•{j+1}: {action} ({score:.2f})")
            
            print(f"  âœ… æ¤œå‡º{i+1}ã®æç”»å®Œäº†")
            
        except Exception as e:
            print(f"  âŒ æ¤œå‡º{i+1}ã®æç”»ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã€ç”»é¢ä¸­å¤®ã«åŸºæœ¬çš„ãªæƒ…å ±ã‚’è¡¨ç¤º
            cv2.rectangle(vis_frame, (100, 100), (500, 300), (0, 255, 0), 4)
            cv2.putText(vis_frame, "Detection Error", (120, 150),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
    
    print(f"ğŸ¨ å¯è¦–åŒ–å®Œäº†")
    return vis_frame

def realtime_action_detection(model, device=None):
    """
    YOLOv8ã‚’ä½¿ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡Œå‹•æ¤œå‡º
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model.to(device)
    model.eval()

    # YOLOv8ã«ã‚ˆã‚‹äººç‰©æ¤œå‡ºå™¨ã®åˆæœŸåŒ–
    try:
        yolo_detector = YOLO("yolov8x.pt")  # æœ€æ–°ã®YOLOv8 extralargeãƒ¢ãƒ‡ãƒ«
        print("YOLOv8xäººç‰©æ¤œå‡ºå™¨ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except Exception as e:
        print(f"YOLOv8xã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€yolov8nã‚’è©¦ã—ã¾ã™: {e}")
        try:
            yolo_detector = YOLO("yolov8n.pt")  # ã‚ˆã‚Šè»½é‡ãªnanoãƒ¢ãƒ‡ãƒ«
            print("YOLOv8näººç‰©æ¤œå‡ºå™¨ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e2:
            print(f"YOLOv8ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e2}")
            print("ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
            return

    # ã‚«ãƒ¡ãƒ©ã‚’é–‹ã
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("ã‚¨ãƒ©ãƒ¼: ã‚«ãƒ¡ãƒ©ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    frame_buffer = []
    last_detections = []
    last_detection_frame = -DETECTION_INTERVAL
    window_width, window_height = 960, 720
    cv2.namedWindow('SlowFast + YOLOv8 Action Detection', cv2.WINDOW_NORMAL)
    cv2.resizeWindow('SlowFast + YOLOv8 Action Detection', window_width, window_height)

    frame_count = 0
    start_time = time.time()
    detection_time = 0

    print("SlowFast + YOLOv8ã«ã‚ˆã‚‹è¡Œå‹•æ¤œå‡ºã‚’é–‹å§‹ã—ã¾ã™ã€‚'q'ã‚­ãƒ¼ã§çµ‚äº†ã—ã¾ã™ã€‚")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("ã‚¨ãƒ©ãƒ¼: ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—ã«å¤±æ•—")
            break

        display_frame = cv2.resize(frame.copy(), (window_width, window_height))
        proc_frame = cv2.resize(frame, (CROP_SIZE, CROP_SIZE))
        frame_buffer.append(proc_frame)
        if len(frame_buffer) > NUM_FRAMES:
            frame_buffer.pop(0)

        should_detect = len(frame_buffer) == NUM_FRAMES and (frame_count - last_detection_frame) >= DETECTION_INTERVAL

        if should_detect:
            print(f"ğŸ” ãƒ•ãƒ¬ãƒ¼ãƒ  {frame_count} ã§æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...")
            det_start = time.time()
            current_frame = cv2.resize(frame, (640, 480))

            # YOLOv8ã«ã‚ˆã‚‹äººç‰©æ¤œå‡º
            try:
                results = yolo_detector.predict(
                    current_frame, 
                    imgsz=640, 
                    conf=PERSON_THRESHOLD,
                    classes=[0],  # äººç‰©ã‚¯ãƒ©ã‚¹ã®ã¿
                    verbose=False
                )
                
                pred_boxes = []
                pred_scores = []
                
                if len(results) > 0 and results[0].boxes is not None:
                    boxes = results[0].boxes
                    for box in boxes:
                        # YOLOv8ã®å‡ºåŠ›å½¢å¼ã«å¯¾å¿œ
                        x1, y1, x2, y2 = map(float, box.xyxy[0].tolist())
                        conf = float(box.conf[0])
                        
                        if conf >= PERSON_THRESHOLD:  # ä¿¡é ¼åº¦é–¾å€¤ãƒã‚§ãƒƒã‚¯
                            # åº§æ¨™ã‚’CROP_SIZEã«åˆã‚ã›ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                            x1 = x1 * CROP_SIZE / current_frame.shape[1]
                            y1 = y1 * CROP_SIZE / current_frame.shape[0]
                            x2 = x2 * CROP_SIZE / current_frame.shape[1]
                            y2 = y2 * CROP_SIZE / current_frame.shape[0]
                            pred_boxes.append([x1, y1, x2, y2])
                            pred_scores.append(conf)

                if pred_boxes:
                    pred_boxes = torch.tensor(pred_boxes, dtype=torch.float32).to(device)
                    pred_scores = torch.tensor(pred_scores, dtype=torch.float32).to(device)
                    print(f"âœ… YOLOv8ã§ {len(pred_boxes)} äººã‚’æ¤œå‡º")
                else:
                    pred_boxes = torch.zeros((0, 4), dtype=torch.float32).to(device)
                    pred_scores = torch.zeros(0, dtype=torch.float32).to(device)
                    print("âš ï¸ YOLOv8ã§äººç‰©æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                print(f"YOLOv8æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
                pred_boxes = torch.zeros((0, 4), dtype=torch.float32).to(device)
                pred_scores = torch.zeros(0, dtype=torch.float32).to(device)

            # äººç‰©ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿è¡Œå‹•èªè­˜ã‚’å®Ÿè¡Œ
            if len(pred_boxes) > 0:
                try:
                    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒãƒƒãƒ•ã‚¡ã‚’SlowFastæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ç”¨ã«å‰å‡¦ç†
                    frames_array = np.array([cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frame_buffer])
                    
                    # å‰å‡¦ç†ã¨å¤‰æ›
                    inputs = preprocess_video_for_detection(frames_array)
                    inputs = [x.to(device) for x in inputs]
                    
                    # è¡Œå‹•äºˆæ¸¬
                    with torch.no_grad():
                        action_preds = model(inputs, pred_boxes)
                    
                    # æ¤œå‡ºçµæœã‚’å‡¦ç†
                    image_size = (CROP_SIZE, CROP_SIZE)
                    new_detections = process_detections(
                        pred_boxes, pred_scores, action_preds,
                        image_size, PERSON_THRESHOLD, ACTION_THRESHOLD
                    )
                    
                    # æ¤œå‡ºçµæœã‚’æ›´æ–°
                    if new_detections:
                        last_detections = new_detections
                        last_detection_frame = frame_count
                        print(f"âœ… æ–°ã—ã„æ¤œå‡ºçµæœã‚’ä¿å­˜: {len(new_detections)}å€‹")
                        
                except Exception as e:
                    print(f"è¡Œå‹•èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print("âš ï¸ äººç‰©ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

            det_end = time.time()
            detection_time += (det_end - det_start)

        # æœ€å¾Œã®æ¤œå‡ºçµæœã‚’è¡¨ç¤ºï¼ˆæŒç¶šè¡¨ç¤ºï¼‰
        if last_detections:
            display_frame = visualize_detections(display_frame, last_detections, TARGET_ACTIONS)
            
            # æ¤œå‡ºçµæœã®å¹´é½¢ã‚’è¡¨ç¤º
            frames_since_detection = frame_count - last_detection_frame
            cv2.putText(display_frame, f"Detection age: {frames_since_detection} frames", (10, 180),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º
        cv2.putText(display_frame, "SlowFast + YOLOv8 Action Detection", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        # ãƒãƒƒãƒ•ã‚¡çŠ¶æ…‹ã®è¡¨ç¤º
        cv2.putText(display_frame, f"Frame buffer: {len(frame_buffer)}/{NUM_FRAMES}", (10, 70),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # æ¤œå‡ºFPSã‚’è¡¨ç¤º
        detection_count = (frame_count // DETECTION_INTERVAL) + 1
        avg_detection_time = detection_time / detection_count if detection_count > 0 else 0
        detection_fps = 1.0 / avg_detection_time if avg_detection_time > 0 else 0
        cv2.putText(display_frame, f"Detection FPS: {detection_fps:.2f}", (10, 110),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # å…¨ä½“ã®FPSã‚’è¡¨ç¤º
        elapsed_time = time.time() - start_time
        fps = frame_count / elapsed_time if elapsed_time > 0 else 0
        cv2.putText(display_frame, f"Display FPS: {fps:.2f}", (10, 150),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºï¼ˆå³ä¸Šï¼‰
        persistent_countdown = max(0, DETECTION_INTERVAL - (frame_count - last_detection_frame))
        status_color = (0, 255, 0) if persistent_countdown > 0 else (0, 0, 255)
        cv2.rectangle(display_frame, (window_width-150, 10), (window_width-10, 130), status_color, 3)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
        status_text = "DETECTING" if should_detect else "WAITING"
        cv2.putText(display_frame, status_text, (window_width-140, 35), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·
        cv2.putText(display_frame, f"F:{frame_count}", (window_width-140, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 2)
        
        # æ¬¡ã®æ¤œå‡ºã¾ã§ã®ã‚«ã‚¦ãƒ³ãƒˆãƒ€ã‚¦ãƒ³
        cv2.putText(display_frame, f"Next:{persistent_countdown}", (window_width-140, 85), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 2)
        
        # æ¤œå‡ºã•ã‚ŒãŸäººæ•°
        num_persons = len(last_detections) if last_detections else 0
        cv2.putText(display_frame, f"Persons:{num_persons}", (window_width-140, 110), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 2)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ è¡¨ç¤º
        cv2.imshow('SlowFast + YOLOv8 Action Detection', display_frame)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ›´æ–°
        frame_count += 1
        
        # 'q'ã‚­ãƒ¼ã§çµ‚äº†
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    # ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
    cap.release()
    cv2.destroyAllWindows()
    
    # æœ€çµ‚çµ±è¨ˆã®è¡¨ç¤º
    total_time = time.time() - start_time
    print(f"\n=== å®Ÿè¡Œçµ±è¨ˆ ===")
    print(f"ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
    print(f"å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {frame_count}")
    print(f"å¹³å‡FPS: {frame_count / total_time:.2f}")
    print(f"æ¤œå‡ºå®Ÿè¡Œå›æ•°: {detection_count}")
    print(f"å¹³å‡æ¤œå‡ºæ™‚é–“: {avg_detection_time:.3f}ç§’")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
    parser = argparse.ArgumentParser(description="SlowFast + YOLOv8 Action Detection System")
    parser.add_argument("--output_dir", type=str, default="./output_slowfast_yolo8_detection",
                        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPUå: {torch.cuda.get_device_name(0)}")
        print(f"GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        print("SlowFastè¡Œå‹•æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        model = SlowFastActionDetectionModel(target_actions=TARGET_ACTIONS)
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºã®å®Ÿè¡Œ
        print("YOLOv8ã«ã‚ˆã‚‹äººç‰©æ¤œå‡ºã¨SlowFastã«ã‚ˆã‚‹è¡Œå‹•èªè­˜ã‚’é–‹å§‹ã—ã¾ã™...")
        realtime_action_detection(model, device)
        
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        print("\næ¨å¥¨ã•ã‚Œã‚‹å¯¾å‡¦æ³•:")
        print("1. PyTorchã¨torchvisionãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª")
        print("2. pytorchvideoãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª")
        print("3. ultralyticsãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª (pip install ultralytics)")
        print("4. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªï¼ˆåˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼‰")

if __name__ == "__main__":
    print("=" * 60)
    print("SlowFast + YOLOv8 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡Œå‹•æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print("ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯æœ€æ–°æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã¾ã™:")
    print("- YOLOv8: æœ€æ–°ã‹ã¤é«˜ç²¾åº¦ãªäººç‰©æ¤œå‡º")
    print("- SlowFast R50: AVAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰è¨“ç·´ã•ã‚ŒãŸè¡Œå‹•èªè­˜")
    print("- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†: Webã‚«ãƒ¡ãƒ©ã‹ã‚‰ã®æ˜ åƒã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è§£æ")
    print()
    print("æ¤œå‡ºå¯èƒ½ãªè¡Œå‹•:")
    for i, action in enumerate(TARGET_ACTIONS):
        print(f"  {i+1:2d}. {action}")
    print()
    print("ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶:")
    print("- Python 3.8+")
    print("- PyTorch")
    print("- pytorchvideo")
    print("- ultralytics (YOLOv8)")
    print("- OpenCV")
    print("- Webã‚«ãƒ¡ãƒ©")
    print()
    print("æº–å‚™ãŒå®Œäº†ã—ãŸã‚‰ä½•ã‹ã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
    input()
    
    main()